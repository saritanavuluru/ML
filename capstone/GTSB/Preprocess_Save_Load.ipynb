{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : REFACTOR\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import sklearn\n",
    "import os\n",
    "#import helper\n",
    "import keras\n",
    "import pickle\n",
    "import math\n",
    "from keras.layers.advanced_activations import LeakyReLU \n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from skimage import exposure\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten,Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Conv2D\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import statistics\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.regularizers import l2\n",
    "\n",
    "import random\n",
    "import csv\n",
    "from tensorflow.contrib.layers import flatten\n",
    "from skimage import transform as transf\n",
    "\n",
    "import loader\n",
    "import preprocess\n",
    "import plotter\n",
    "\n",
    "\n",
    "#from prettytable import PrettyTable\n",
    "%matplotlib inline\n",
    "\n",
    "SEED = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test,X_val,y_val = loader.load_split_input_data()\n",
    "X_train_g, X_test_g,X_val_g = preprocess.convert_to_grayscale(X_train, X_test,X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train_h, X_test_h, X_val_h = preprocess.hist_equalize_set(X_train_g, X_test_g, X_val_g)\n",
    "#X_train_n, X_test_n,X_val_n = preprocess.noralize(X_train_g, X_test_g,X_val_g)\n",
    "X_train_hae ,X_test_hae, X_val_hae = preprocess.adaptive_equalize_set(X_train_h, X_test_h, X_val_h)\n",
    "X_train_hcs,X_test_hcs,X_val_hcs = preprocess.contrast_stretch_set(X_train_h,X_test_h,X_val_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./preprocessed/preprocessed_hist_gray.p\n",
      "X_train:  34799\n",
      "y_train:  34799\n",
      "X_val:  4410\n",
      "y_val:  4410\n",
      "X_test:  12630\n",
      "y_test:  12630\n",
      "X_aug:  8600\n",
      "y_aug:  8600\n",
      "Data and modules loaded.\n",
      "./preprocessed/preprocessed_hae_gray.p\n",
      "X_train:  34799\n",
      "y_train:  34799\n",
      "X_val:  4410\n",
      "y_val:  4410\n",
      "X_test:  12630\n",
      "y_test:  12630\n",
      "X_aug:  8600\n",
      "y_aug:  8600\n",
      "Data and modules loaded.\n",
      "./preprocessed/preprocessed_hcs_gray.p\n",
      "X_train:  34799\n",
      "y_train:  34799\n",
      "X_val:  4410\n",
      "y_val:  4410\n",
      "X_test:  12630\n",
      "y_test:  12630\n",
      "X_aug:  8600\n",
      "y_aug:  8600\n",
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "X_train_h,y_train,X_test_h,y_test,X_val_h,y_val,X_aug_h,y_aug_h = loader.load_preprocessed_data(\"./preprocessed/preprocessed_hist_gray.p\")\n",
    "X_train_hae,y_train,X_test_hae,y_test,X_val_hae,y_val,X_aug_hae,y_aug_hae = loader.load_preprocessed_data(\"./preprocessed/preprocessed_hae_gray.p\")\n",
    "X_train_hcs,y_train,X_test_hcs,y_test,X_val_hcs,y_val,X_aug_hcs,y_aug_hcs = loader.load_preprocessed_data(\"./preprocessed/preprocessed_hcs_gray.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combining dataset 0 of type <class 'numpy.ndarray'> and length 34799\n",
      "combining dataset 1 of type <class 'numpy.ndarray'> and length 34799\n",
      "combining dataset 2 of type <class 'list'> and length 34799\n",
      "Dataset 2 is a list; converting to  ndarray\n",
      "length of combined set : 104397\n"
     ]
    }
   ],
   "source": [
    "def combine_datasets(datasets):\n",
    "\n",
    "    x=datasets[0][0]\n",
    "    y = datasets[0][1]\n",
    "    \n",
    "    if type(x) is list:\n",
    "            x = np.asarray(x)\n",
    "            print(str(count)+' is a list')\n",
    "    if type(y) is list:\n",
    "            y = np.asarray(y)\n",
    "        \n",
    "    #print(x.shape)\n",
    "    #print(y.shape)\n",
    "    X_all=x\n",
    "    y_all=y\n",
    "    print('combining dataset '+str(0)+' of type '+str(type(x))+' and length '+str(len(x)))\n",
    "            \n",
    "    count = 0\n",
    "    for ds in datasets:\n",
    "        if count==0: #since we added the first one already\n",
    "            count+=1\n",
    "            continue\n",
    "       \n",
    "        #print(type(ds))\n",
    "        x = ds[0]\n",
    "        y=ds[1]\n",
    "        print('combining dataset '+str(count)+' of type '+str(type(x))+' and length '+str(len(x)))\n",
    "        if type(x) is list:\n",
    "            x = np.asarray(x)\n",
    "            print('Dataset '+str(count)+' is a list; converting to  ndarray')\n",
    "        if type(y) is list:\n",
    "            y = np.asarray(y)\n",
    "        \n",
    "        \n",
    "\n",
    "        X_all=np.append(X_all,x,axis=0)\n",
    "        y_all=np.append(y_all,y,axis=0)\n",
    "        count+=1\n",
    "\n",
    "\n",
    "   \n",
    "    assert(len(X_all) == len(y_all))\n",
    "    print('length of combined set : '+str(len(X_all)))\n",
    "    return np.asarray(X_all),np.asarray(y_all)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#X_all,y_all=combine_datasets([(X_train_h,y_train),(X_train_hae,y_train),(X_train_hcs,y_train)(X_train_n,y_train)])\n",
    "X_all,y_all=combine_datasets([(X_train_h,y_train),(X_train_hae,y_train),(X_train_hcs,y_train)])\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_augment(X,y,n_classes,n_gen_per_class,n_aug_per_class):\n",
    "\n",
    "    print('Generating '+str(n_gen_per_class)+' number of images per class, selecting '+str(n_aug_per_class)+' number of images per class for  augmentation')\n",
    "    \n",
    "    datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    fill_mode='nearest',\n",
    "   # horizontal_flip=True,\n",
    "    )\n",
    "\n",
    "    \n",
    "    img_shape = [X.shape[1], X.shape[2], X.shape[3]]\n",
    "    img_shape.insert(0,0)\n",
    "    \n",
    "    data_shape = np.asarray(img_shape)\n",
    "    total_image_per_class = n_gen_per_class\n",
    "    \n",
    "    X_augmented =np.empty(data_shape)\n",
    "    y_augmented =np.empty(0,dtype='uint8')\n",
    "    \n",
    "\n",
    "\n",
    "    print('Augmenting  Data...')\n",
    "    for i in tqdm(range(n_classes)):\n",
    "    #for i in tqdm(range(3)):\n",
    "            #print(i)\n",
    "            index = [y==i]\n",
    "            images_for_i_class = X[y==i]\n",
    "            y_i_class = y[y==i]\n",
    "            X_augmented_i = np.empty(data_shape)\n",
    "            #print(X_augmented_i.shape)\n",
    "            y_augmented_i = np.empty(0,dtype='uint8')\n",
    "            for X_b,y_b in datagen.flow(images_for_i_class, y_i_class, batch_size=len(y_i_class), seed=9345+i*37):            \n",
    "                X_augmented_i = np.append(X_augmented_i, X_b, axis=0)\n",
    "                y_augmented_i = np.append(y_augmented_i, y_b, axis=0)\n",
    "\n",
    "                if len(X_augmented_i) >= total_image_per_class:\n",
    "                    break\n",
    "            X_augmented_i, y_augmented_i = shuffle(X_augmented_i, y_augmented_i, random_state=9345)\n",
    "            X_augmented = np.append(X_augmented, X_augmented_i[:n_aug_per_class], axis=0)\n",
    "            y_augmented = np.append(y_augmented, y_augmented_i[:n_aug_per_class], axis=0)     \n",
    "    print(\"shufle\")\n",
    "    X_augmented, y_augmented = shuffle(X_augmented, y_augmented, random_state=9345)\n",
    "    print(\"X_augmented shape: \"+str(X_augmented.shape))\n",
    "    print(\"y_augmented shape: \"+str(y_augmented.shape))\n",
    "    return X_augmented, y_augmented\n",
    "    \n",
    "    # Storing for checkpoint2\n",
    "    #X_augmented = X_augmented.astype('float32')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 1000 number of images per class, selecting 200 number of images per class for  augmentation\n",
      "Augmenting  Data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/43 [00:00<?, ?it/s]\n",
      "  2%|▏         | 1/43 [00:00<00:11,  3.54it/s]\n",
      "  5%|▍         | 2/43 [00:01<00:27,  1.49it/s]\n",
      "  7%|▋         | 3/43 [00:03<00:36,  1.09it/s]\n",
      "  9%|▉         | 4/43 [00:04<00:35,  1.11it/s]\n",
      " 12%|█▏        | 5/43 [00:05<00:40,  1.05s/it]\n",
      " 14%|█▍        | 6/43 [00:06<00:39,  1.07s/it]\n",
      " 16%|█▋        | 7/43 [00:06<00:29,  1.21it/s]\n",
      " 19%|█▊        | 8/43 [00:07<00:29,  1.19it/s]\n",
      " 21%|██        | 9/43 [00:08<00:29,  1.14it/s]\n",
      " 23%|██▎       | 10/43 [00:09<00:27,  1.21it/s]\n",
      "100%|██████████| 43/43 [00:25<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shufle\n",
      "X_augmented shape: (8600, 32, 32, 1)\n",
      "y_augmented shape: (8600,)\n",
      "(8600, 32, 32, 1)\n",
      "(8600,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_aug_all, y_aug_all = preprocess.data_augment(X_all,y_all,n_classes=43,n_gen_per_class=1000,n_aug_per_class=200)\n",
    "print(X_aug_all.shape)\n",
    "print(y_aug_all.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessed/preprocessed_hset_aug_gray.p\n",
      "Saving data to preprocessed/preprocessed_hset_aug_gray.p file...\n",
      "Data cached in preprocessed/preprocessed_hset_aug_gray.p\n"
     ]
    }
   ],
   "source": [
    "from imp import reload\n",
    "\n",
    "loader = reload(loader)\n",
    "\n",
    "loader.save_to_disk(X_all,y_all,X_test_hae,y_test,X_val_hae,y_val,X_aug_all,y_aug_all,False,\"hset_aug\")\n",
    "\n",
    "#save_aug_all_to_disk(X_aug_all,y_aug_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./preprocessed/preprocessed_hset_aug_gray.p\n",
      "X_train:  104397\n",
      "y_train:  104397\n",
      "X_val:  4410\n",
      "y_val:  4410\n",
      "X_test:  12630\n",
      "y_test:  12630\n",
      "X_aug:  8600\n",
      "y_aug:  8600\n",
      "Data and modules loaded.\n"
     ]
    }
   ],
   "source": [
    "X_all,y_all,X_test_hae,y_test,X_val_hae,y_val,X_aug_all,y_aug_all = loader.load_preprocessed_data(\"./preprocessed/preprocessed_hset_aug_gray.p\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combining dataset 0 of type <class 'numpy.ndarray'> and length 104397\n",
      "combining dataset 1 of type <class 'numpy.ndarray'> and length 8600\n",
      "length of combined set : 112997\n",
      "Augmented Combined Datasets:112997\n",
      "Combined Datasets:104397\n"
     ]
    }
   ],
   "source": [
    "X_aug_all,y_aug_all=combine_datasets([(X_all,y_all),(X_aug_all,y_aug_all)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#y_train_e = np_utils.to_categorical(y_train,num_classes=43)\n",
    "\n",
    "n_classes = 43\n",
    "\n",
    "y_all_e = np_utils.to_categorical(y_all,num_classes=n_classes)\n",
    "y_aug_all_e = np_utils.to_categorical(y_aug_all,num_classes=n_classes)\n",
    "y_test_e = np_utils.to_categorical(y_test, num_classes=n_classes)\n",
    "y_val_e = np_utils.to_categorical(y_val, num_classes=n_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tf_a",
   "language": "python",
   "name": "tf_a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
